{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0ipU50JPGoN",
        "outputId": "efb6f44c-7e6b-40b0-b11a-f76da12fd922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKPJPWC6LEbt",
        "outputId": "da988bf0-c874-483a-ee6f-316129447dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-389e4hq4/unsloth_93223b989c7d43b2a9b4245279e91a17\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-389e4hq4/unsloth_93223b989c7d43b2a9b4245279e91a17\n"
          ]
        }
      ],
      "source": [
        "# Install Unsloth (if not already installed)\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install \"unsloth [colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install xformers \"trl<0.9.0\" peft accelerate bitsandbytes datasets\n",
        "# !pip uninstall transformers\n",
        "# !pip install git+https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
        "import unsloth\n",
        "\n",
        "# Clear unused CUDA memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "# Define model ID\n",
        "model_id = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
        "ds = load_dataset(\"rajistics/indian_food_images\")\n",
        "\n",
        "\n",
        "# Load model with Unsloth for optimized execution\n",
        "# with unsloth.optimize():\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "       model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Download and load the image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow\n",
        "from PIL import Image\n",
        "import requests\n",
        "url = \"https://files.catbox.moe/rykgsb.jpeg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Prepare the prompt and inputs\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Detect and give output its food quality in terms of 0 to 1 and give it a boolean value of eatable.Give only in format : quality - 0 to 1 & boolean - true or false\",\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z03zdsgdLb4K",
        "outputId": "5667f685-0be5-41b4-bddb-3cca2629d347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(image, input_text, add_special_tokens=True, return_tensors=\"pt\").to(\n",
        "    model.device\n",
        ")\n",
        "\n",
        "# Generate the output\n",
        "output = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode and print the output\n",
        "# print(processor.decode(output[0]))\n",
        "decoded_output = processor.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "W77t-YbZTCfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "O_W0S6ejzvAo",
        "outputId": "c40e1a17-ded4-472c-8951-c2824cfabc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'user\\n\\nDetect and give output its food quality in terms of 0 to 1 and give it a boolean value of eatable.Give only in format : quality - 0 to 1 & boolean - true or falseassistant\\n\\nTo determine the food quality and whether it is eatable, we need to analyze the image provided. The image shows a hand holding a piece of bread with some filling inside. However, the image is blurry, making it difficult to identify the exact type'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "quality_match = re.search(r\"quality\\s*-\\s*(\\d\\.\\d+)\", decoded_output)\n",
        "boolean_match = re.search(r\"boolean\\s*-\\s*(true|false)\", decoded_output, re.IGNORECASE)\n",
        "\n",
        "# Final structured output\n",
        "quality = float(quality_match.group(1)) if quality_match else None\n",
        "eatable = boolean_match.group(1).lower() == \"true\" if boolean_match else None\n",
        "\n",
        "print(f\"Quality: {quality}, Eatable: {eatable}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzuIvMPZzz2S",
        "outputId": "7d5630f8-215f-4d31-f9b5-006866831dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality: None, Eatable: True\n"
          ]
        }
      ]
    }
  ]
}